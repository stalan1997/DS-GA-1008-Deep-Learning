# -*- coding: utf-8 -*-
"""mlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19mjJ7vzrWsVAADwkFmnhzZs-w6nlcI2J
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class MLP:
    def __init__(
        self,
        linear_1_in_features,
        linear_1_out_features,
        f_function,
        linear_2_in_features,
        linear_2_out_features,
        g_function
    ):
        """
        Args:
            linear_1_in_features: the in features of first linear layer
            linear_1_out_features: the out features of first linear layer
            linear_2_in_features: the in features of second linear layer
            linear_2_out_features: the out features of second linear layer
            f_function: string for the f function: relu | sigmoid | identity
            g_function: string for the g function: relu | sigmoid | identity
        """
        self.f_function = f_function
        self.g_function = g_function

        self.parameters = dict(
            W1 = torch.randn(linear_1_out_features, linear_1_in_features),
            b1 = torch.randn(linear_1_out_features),
            W2 = torch.randn(linear_2_out_features, linear_2_in_features),
            b2 = torch.randn(linear_2_out_features),
        )
        self.grads = dict(
            dJdW1 = torch.zeros(linear_1_out_features, linear_1_in_features),
            dJdb1 = torch.zeros(linear_1_out_features),
            dJdW2 = torch.zeros(linear_2_out_features, linear_2_in_features),
            dJdb2 = torch.zeros(linear_2_out_features),
        )

        # put all the cache value you need in self.cache
        self.cache = dict(
        )

    def forward(self, x):
        """
        Args:
            x: tensor shape (batch_size, linear_1_in_features)
        """
        # TODO: Implement the forward function
        w1 = self.parameters['W1']
        b1 = self.parameters['b1']
        w2 = self.parameters['W2']
        b2 = self.parameters['b2']
        
        self.z1 = w1.mm(x.t())
        for i in range(10):
          self.z1[:,i] = self.z1[:,i] + b1

        if self.f_function == 'identity':
          self.z2 = self.z1
        elif self.f_function == 'relu':
          self.z2 = F.relu_(self.z1)
        else:
          self.z2 = F.sigmoid(self.z1)
        
        self.z3 = w2.mm(self.z2)
        for i in range(10):
          self.z3[:,i] = self.z3[:,i] + b2
        if self.g_function == 'relu':
          self.y_hat = F.relu_(self.z3)
        elif self.g_function == 'identity':
          self.y_hat = self.z3
        elif self.g_function == 'sigmoid':
          self.y_hat = F.sigmoid(self.z3)
        return self.y_hat

        pass
    
    def backward(self, dJdy_hat):
        """
        Args:
            dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)
        """
        # TODO: Implement the backward function
        if self.g_function == 'identity':
          dy_hatdz3 = 1
        elif self.g_function == 'sigmoid':
          a = 1 - F.sigmoid(self.z3)
          dy_hatdz3 = F.sigmoid(self.z3) * a
        elif self.g_function == 'relu':
          dy_hatdz3 = torch.sign(F.relu_(self.z3))
        
        self.grads['dJdW2'] = (dJdy_hat * dy_hatdz3).mm(self.z2.t())/dJdy_hat.size(0)/dJdy_hat.size(1)
        self.grads['dJdb2'] = (dJdy_hat * dy_hatdz3).sum(1)/dJdy_hat.size(0)/dJdy_hat.size(1)


        dz3dz2 = self.parameters['W2']


        if self.f_function == 'identity':
          dz2dz1 = 1
        elif self.f_function == 'sigmoid':
          b = 1 - F.sigmoid(self.z1)
          dz2dz1 = F.sigmoid(self.z1) * b
        elif self.f_function == 'relu':
          dz2dz1 = torch.sign(F.relu_(self.z1))
        
        self.grads['dJdW1'] = (dz3dz2.t().mm(dJdy_hat * dy_hatdz3) * dz2dz1).mm(x)/dJdy_hat.size(0)/dJdy_hat.size(1)
        self.grads['dJdb1'] = (dz3dz2.t().mm(dJdy_hat * dy_hatdz3) * dz2dz1).sum(1)/dJdy_hat.size(0)/dJdy_hat.size(1)


        pass

    
    def clear_grad_and_cache(self):
        for grad in self.grads:
            self.grads[grad].zero_()
        self.cache = dict()

def mse_loss(y, y_hat):
    """
    Args:
        y: the label tensor (batch_size, linear_2_out_features)
        y_hat: the prediction tensor (batch_size, linear_2_out_features)

    Return:
        J: scalar of loss
        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)
    """
    # TODO: Implement the mse loss
    loss_matrix = 0.5 * (y.t()-y_hat).pow(2).sum(1)
    dim = y.size(0)
    J = loss_matrix/dim
    dJdy_hat = y_hat - y.t()
    return J, dJdy_hat
    pass


    # return loss, dJdy_hat

def bce_loss(y, y_hat):
    """
    Args:
        y_hat: the prediction tensor
        y: the label tensor
        
    Return:
        loss: scalar of loss
        dJdy_hat: The gradient tensor of shape (batch_size, linear_2_out_features)
    """
    # TODO: Implement the bce loss
    log_1 = torch.clamp(torch.log(y_hat), min = -100)
    log_2 = torch.clamp(torch.log(1 - y_hat), min = -100)
    loss_matrix = -(y.t() * log_1 + (1 - y.t()) * log_2)
    dim = y.size(0)
    J = loss_matrix.sum(1)/dim
    dJdy_hat = (1-y.t())/(1-y_hat) - y.t()/y_hat
    return J, dJdy_hat
    pass

"""Author: Yilang Hao
NetID: yh3062
"""